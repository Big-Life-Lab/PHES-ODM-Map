---
toc: true
toc-depth: 2
css: assets/styles.css
---

```{r, echo=FALSE, eval=TRUE, file="../R/doc_mdtables.R"}
```

# Mapping operations

This document lists all operations allowable in a mapping configuration file. For details on the mapping configuration file see the [Mapping Configuration Files](mapping-configs.html) document.

# copy

```yaml
operation: copy
operation_config:
    target_column: mr_reportDate
    source_table: measures
    source_column: reportDate
    target_index: all           # 'all', int, or List[int], default: 'all'
```

The `copy` operation copies a value from a column in the source table to a column in the output table. If the output currently has more than one row the row index to copy the value to can be set with the `target_index` key, which is a 0-based index. `target_index` can also be an array of integers to specify multiple row indices to copy to. If `target_index` is not specified, or `all` is specified, then all rows are copied to. The default value is `all`.

To avoid having to enter many separate `copy` operations, multiple `copy` operations can be specified using arrays for `target_column`, `source_table`, and `source_column`. The example below copies `samples["siteID"]` to the output's `sm_siteID` column, `samples["saMaterial"]` to the output's `sm_sampleMat` column, and `contacts["contactID"]` to the output's `co_contactID` column:

```yaml
operation: copy
operation_config:
    target_column: [ sm_siteID, sm_sampleMat, co_contactID ]
    source_table: [ samples, samples, contacts ]
    source_column: [ siteID, saMaterial, contactID ]
```

If multiple output rows exist, then the `copy` operation will cycle through the values in the input rows for each subsequent output row. This ensures that we maintain the property that output row i corresponds to input row i mod n (i % n), where n is the number of input rows.

# order_columns

```yaml
operation: order_columns
operation_config:
    column_order_regex: [ mr_reportDate, sm_siteID, sm_sampleMat, co_contactID, compartment, specimen, fraction_analysed, measure, unit, aggregation, index, .* ]
```

The `order_columns` operation reorders the columns in the output table. The list of strings `column_order_regex` specifies the order of the columns. These are regular expressions that are applied to all columns. Full matches must occur in order for a regular expression to match a column name.

If an exact match occurs (eg. the column `mr_reportDate` matching the regular expression `mr_reportDate`) then the column is placed in the index position of that match. Otherwise, if no exact match occurs then the index position of a wildcard match is used. If multiple columns match a wildcard then those columns are sorted alphabetically for consistency. If no match occurs then the column is placed at the end. For example, if a table has a column `mr_reportDate`, `mr_specimen`, `mr_aggregation`, and `sm_sampleMat`, and the following configuration is used:

```yaml
operation: order_columns
operation_config:
    column_order_regex: [ mr_specimen, mr_.*, mr_aggregation ]
```

Then the columns will be reordered so that `mr_specimen` is first, `mr_reportDate` is second, `mr_aggregation` is third, and `sm_sampleMat` is last.

# pivot_longer

```yaml
operation: pivot_longer
operation_config:
    match_column_values: ([^_]+)_([^_]+)_([^_]+)_([^_]+)_([^_]+)_([^_]+)_([^_]+)_value
    match_skip_values: [ hCr, hSp, hFr, hMr, hUn, [ hAg, hAG ], hIn ]
    target_columns: [ compartment, specimen, fraction_analysed, measure, unit, aggregation, index ]
    target_value_column: value
    source_table: measures
```

The `pivot_longer` operation pivots individual input columns into individual output rows. It is the inverse of the `pivot_wider` operation.

From the input rows (of the input table specified by `source_table`) `pivot_longer` finds any column that fully matches the regular expression specified in `match_column_values`. The regular expression must match the full column name, not just a part of it. It then takes the captures in the regular expression and assigns them to the columns (in order) specified in `target_columns`. In the `target_columns` field a value of `NULL` can be used if that capture should be ignored.  It also takes the value in the input table found in the matched column and assigns it to the column specified in `target_value_column`. Using the input row below as an example:

```{r, echo=FALSE} 
mdtable_list(list(
    date = "2024-12-01",
    wat_sa_liq_covN1_gcL_me_1_value = "40",
    wat_sa_liq_pmmov_gcL_me_1_value = "45",
    wat_sa_liq_ph_unitless_me_1_value = "6.1"
))
```

All columns except for the `date` column match the regular expression. The first column that matches is `wat_sa_liq_covN1_gcL_me_1_value`, with the first regular expression capture being `wat`, the second `sa`, the third `liq`, and so on. Using `target_columns` to assign these captures to column values, and assigning the source value to the column specified by `target_value_column`, we obtain the following row:

```{r, echo=FALSE}
mdtable_list(list(
    compartment = "wat",
    specimen = "sa",
    fraction_analysed = "liq",
    measure = "covN1",
    unit = "gcL",
    aggregation = "me",
    index = "1",
    value = "40"
))
```

Repeating this with the other columns that match the regular expression, we obtain:

```{r, echo=FALSE}
mdtable_list(list(
    compartment = c("wat", "wat", "wat"),
    specimen = c("sa", "sa", "sa"),
    fraction_analysed = c("liq", "liq", "liq"),
    measure = c("covN1", "pmmov", "ph"),
    unit = c("gcL", "gcL", "unitless"),
    aggregation = c("me", "me", "me"),
    index = c("1", "1", "1"),
    value = c("40", "45", "6.1")
))
```

The `match_skip_values` field specifies values of captures that should result in skipping a pivot longer operation for a matched column. This list can have elements that are either strings or sub-lists of strings. If an element is a string then matching that string to the capture for that index will skip the operation. If an element is a sub-list then matching any of the strings in the sub-list will skip the operation. For example, in the example given at the start of this section, if the sixth capture (for `aggregation`) is either `hAg` or `hAG` then the pivot longer operation will be skipped. Use a value of `NULL` if there are no skip matches for that capture.

When running `pivot_longer`, each matched column will result in populating a different output row, starting at output row index 0. If rows already exist in the output, we will populate those existing rows first, one at a time. If there are not enough output rows, then new output rows will be created. When creating the new rows the last pre-existing row in the output is broadcast down to populate any columns that are not populated by the `pivot_longer` operation (eg. if in the last pre-existing output row a `date` column is already present and has a value of `2024-12-01`, then any new row created by `pivot_longer` will also have a `date` of `2024-12-01`).

When pivoting longer, once a column to pivot has been identified, we pivot all input rows in order for that particular column, appending an additional output row for each pivoted input row. Only after all input rows are pivoted for the column do we proceed to the next column for pivoting. This ensures that we maintain the property that output row i corresponds to input row i mod n (i % n), where n is the number of input rows.

# pivot_wider

```yaml
operation: pivot_wider
operation_config:
    target_column_sources: [ compartment, specimen, fraction_analysed, measure, unit, aggregation, index ]
    target_column_separator: _
    target_column_suffix: _value
    source_table: measures
    source_column: value
```

The `pivot_wider` operation pivots individual input rows into individual output columns. It is the inverse of the `pivot_longer` operation. When mapping a single input row to a column we take the values in various columns of the input row (specified by `target_column_sources`), concatenate those values together as strings to construct the new output column name (separated by `target_column_separator`), optionally append a prefix or suffix to the name (specified by `target_column_prefix` or `target_column_suffix`), and then copy over a value from the input row (in the column specified by `source_column`) to the new column.

In `target_column_sources`, a dictionary value of the form `{ constant: hAg }` can be specified to use the constant string `hAg` in the column name, rather than a value retrieved from the input.

Using the example configuration above and applying it to the following rows:

```{r, echo=FALSE}
mdtable_csv_file("../assets/examples/odm/measure-singledate-odmlong.csv")
```

We will first pivot the first row into a single column. The values within the columns specified by `target_column_sources` are:

```txt
(wat, sa, liq, covN1, gcL, me, 1)
```

Using `target_column_separator` as the separator, we concatenate the values to obtain:

```txt
wat_sa_liq_covN1_gcL_me_1
```

Finally, we append the `target_column_suffix` to obtain:

```txt
wat_sa_liq_covN1_gcL_me_1_value
```

This is the column name in the output. Finally, we copy the value found in the `source_column` column of the table `source_table`, which is 40. We repeat this process for rows two and three to obtain:

```{r, echo=FALSE} 
mdtable_list(list(
    wat_sa_liq_covN1_gcL_me_1_value = "40",
    wat_sa_liq_pmmov_gcL_me_1_value = "45",
    wat_sa_liq_ph_unitless_me_1_value = "6.1"
))
```

# set

```yaml
operation: set
operation_config:
    target_column: firstName
    target_value: Martin
    target_index: all           # 'all', int, or List[int], default: 'all'
```

The `set` operation assigns predefined values to columns in the output rows. If the output currently has more than one row the row index to copy the value to can be set with the `target_index` key, which is a 0-based index. `target_index` can also be an array of integers to specify multiple row indices to set. If `target_index` is not specified, or `all` is specified, then all rows are set. The default value is `all`.

To avoid having to enter many separate `set` operations, multiple `set` operations can be specified using arrays for `target_column` and `target_value`. The example below sets the `organizationID` column in the output rows to `ohriOrg123`, `firstName` to `Martin`, `lastName` to `Wellman`, and `email` to `mwellman@example.org`:

```yaml
operation: set
operation_config:
    target_column: [ organizationID, firstName, lastName, email ]
    target_value: [ ohriOrg123, Martin, Wellman, mwellman@example.org ]
```

# sort_rows

```yaml
operation: sort_rows
operation_config:
    sort_by: [ date, sampleID ]
    order: ascending
```

The `sort_rows` operation sorts all rows according to one or more columns. This operation is typically performed as a post operation (in the `post_operations` key for the table, rather than with the `operations` key). It can be performed in the `operations` key but the sorting will only be performed on the current set of output rows, which is just a subset of the final output table. The configuration's `sort_by` field can be a single column name, or a list of column names to sort by. If this is a list then earlier columns take precedence in sorting. ie. We sort the whole table by the last column in the list first, and the first column in the list last.
